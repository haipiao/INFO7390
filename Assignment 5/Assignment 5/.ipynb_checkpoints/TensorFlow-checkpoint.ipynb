{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CsvDataset shapes: ((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()), types: (tf.int32, tf.string, tf.string, tf.int32, tf.string, tf.string, tf.string, tf.string, tf.int32, tf.string, tf.string, tf.string, tf.int32, tf.int32, tf.string, tf.string, tf.string, tf.string, tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=['myMovies.csv']\n",
    "record_defaults=[[0],['null'],['null'],[-1],['null'],['en'],['null'],['null'],[0],['null'],['null'],['0000-00-00'],[0],[0],['null'],['Released'],['null'],['null'],[0.0],[0]]\n",
    "dataset = tf.contrib.data.CsvDataset(filename,record_defaults,header=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffleset = dataset.shuffle(buffer_size=100)\n",
    "train_set = shuffleset.take(4000)\n",
    "test_set = shuffleset.skip(800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()), types: (tf.int32, tf.string, tf.string, tf.int32, tf.string, tf.string, tf.string, tf.string, tf.int32, tf.string, tf.string, tf.string, tf.int32, tf.int32, tf.string, tf.string, tf.string, tf.string, tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "x = tf.placeholder(tf.float32, [None, 20])\n",
    "# declaring the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 4])\n",
    "# declaring the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([20, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 4], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([4]), name='b2')\n",
    "# calculating the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)\n",
    "# calculating the hidden layer output using softmax activated\n",
    "# output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                        + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "# adding an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# defining an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(200 / batch_size)\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _ , c = sess.run([optimiser, cross_entropy], \n",
    "                         feed_dict={x: train_set, y: train_set})\n",
    "            avg_cost += c / total_batch\n",
    "        test_acc = sess.run(accuracy,feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \"test accuracy: {:.3f}\".format(test_acc))\n",
    "   print(\"\\nTraining complete!\")     \n",
    "   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63440738, 0.98198239],\n",
       "       [0.71840113, 0.70405658],\n",
       "       [0.79419558, 0.00279968],\n",
       "       [0.62682698, 0.84243399],\n",
       "       [0.97848694, 0.99919005],\n",
       "       [0.1568789 , 0.12741069],\n",
       "       [0.45131105, 0.32773236],\n",
       "       [0.58679491, 0.29034558],\n",
       "       [0.6641515 , 0.98727736],\n",
       "       [0.10906229, 0.02332562],\n",
       "       [0.03379221, 0.67895272],\n",
       "       [0.17342023, 0.48118316],\n",
       "       [0.19972312, 0.44856592],\n",
       "       [0.36290937, 0.94583797],\n",
       "       [0.34926449, 0.93661887],\n",
       "       [0.09750308, 0.43424575],\n",
       "       [0.45393067, 0.08979345],\n",
       "       [0.59286445, 0.52378201],\n",
       "       [0.55823297, 0.4700243 ],\n",
       "       [0.33048   , 0.50748282],\n",
       "       [0.5365893 , 0.08157501],\n",
       "       [0.52007664, 0.37324983],\n",
       "       [0.2353004 , 0.67484483],\n",
       "       [0.08368932, 0.72034624],\n",
       "       [0.96160024, 0.22250831],\n",
       "       [0.42745969, 0.63688935],\n",
       "       [0.3009002 , 0.82872294],\n",
       "       [0.17651632, 0.63900972],\n",
       "       [0.93940633, 0.34200228],\n",
       "       [0.35637439, 0.25438107],\n",
       "       [0.39475033, 0.11816209],\n",
       "       [0.57399585, 0.30058617],\n",
       "       [0.64024016, 0.04790027],\n",
       "       [0.9634955 , 0.07183922],\n",
       "       [0.07259525, 0.05976443],\n",
       "       [0.44318055, 0.54495382],\n",
       "       [0.1806945 , 0.79988269],\n",
       "       [0.5118704 , 0.25948408],\n",
       "       [0.19433388, 0.90007821],\n",
       "       [0.40957451, 0.72641865],\n",
       "       [0.54188963, 0.86732854],\n",
       "       [0.30042683, 0.0866424 ],\n",
       "       [0.13657698, 0.47025082],\n",
       "       [0.94833859, 0.46655279],\n",
       "       [0.12752248, 0.69861481],\n",
       "       [0.32851147, 0.13020215],\n",
       "       [0.28100353, 0.97509379],\n",
       "       [0.83640664, 0.38924151],\n",
       "       [0.99191864, 0.03501644],\n",
       "       [0.04041981, 0.10956148],\n",
       "       [0.27963254, 0.47845916],\n",
       "       [0.10860814, 0.25873094],\n",
       "       [0.4104053 , 0.64828871],\n",
       "       [0.84486675, 0.47581577],\n",
       "       [0.99439518, 0.64159761],\n",
       "       [0.40859731, 0.43392092],\n",
       "       [0.30150485, 0.05388216],\n",
       "       [0.82551115, 0.62124459],\n",
       "       [0.00682495, 0.44244366],\n",
       "       [0.576444  , 0.32381946],\n",
       "       [0.4313585 , 0.89511326],\n",
       "       [0.90522696, 0.88598668],\n",
       "       [0.74292415, 0.72539189],\n",
       "       [0.50424119, 0.66341816],\n",
       "       [0.24476873, 0.24651975],\n",
       "       [0.63842677, 0.51737076],\n",
       "       [0.21096471, 0.70731966],\n",
       "       [0.76029899, 0.64054918],\n",
       "       [0.40814505, 0.81264234],\n",
       "       [0.04645431, 0.04510132],\n",
       "       [0.75114773, 0.36511195],\n",
       "       [0.99210784, 0.15024397],\n",
       "       [0.0331779 , 0.91988987],\n",
       "       [0.93641323, 0.89447076],\n",
       "       [0.37020165, 0.30259975],\n",
       "       [0.40700154, 0.27611076],\n",
       "       [0.95562613, 0.35390576],\n",
       "       [0.5641922 , 0.20461976],\n",
       "       [0.75780604, 0.89196574],\n",
       "       [0.76312975, 0.89301753],\n",
       "       [0.91234182, 0.84057466],\n",
       "       [0.30949932, 0.32649568],\n",
       "       [0.9144645 , 0.90613573],\n",
       "       [0.55773847, 0.092138  ],\n",
       "       [0.07197   , 0.13381119],\n",
       "       [0.68260316, 0.70643687],\n",
       "       [0.6465239 , 0.58538864],\n",
       "       [0.58260639, 0.71869026],\n",
       "       [0.5802672 , 0.76417717],\n",
       "       [0.11903729, 0.56819394],\n",
       "       [0.89332751, 0.20872476],\n",
       "       [0.53453616, 0.09835098],\n",
       "       [0.2215904 , 0.04100978],\n",
       "       [0.45521429, 0.49319482],\n",
       "       [0.47459278, 0.59806432],\n",
       "       [0.28113168, 0.12457464],\n",
       "       [0.43657417, 0.89906877],\n",
       "       [0.67804286, 0.29564202],\n",
       "       [0.5885057 , 0.44584797],\n",
       "       [0.56315583, 0.92164591]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.sample((100,2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (2,), types: tf.float64>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>237000000</td>\n",
       "      <td>['Action', 'Adventure', 'Fantasy', 'Science Fi...</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>19995</td>\n",
       "      <td>['culture clash', 'future', 'space war', 'spac...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>...</td>\n",
       "      <td>['United States of America', 'United Kingdom']</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2787965087</td>\n",
       "      <td>162.0</td>\n",
       "      <td>['English', 'Español']</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     budget                                             genres  \\\n",
       "0           0  237000000  ['Action', 'Adventure', 'Fantasy', 'Science Fi...   \n",
       "\n",
       "                      homepage     id  \\\n",
       "0  http://www.avatarmovie.com/  19995   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  ['culture clash', 'future', 'space war', 'spac...                en   \n",
       "\n",
       "  original_title                                           overview  \\\n",
       "0         Avatar  In the 22nd century, a paraplegic Marine is di...   \n",
       "\n",
       "   popularity     ...                                production_countries  \\\n",
       "0  150.437577     ...      ['United States of America', 'United Kingdom']   \n",
       "\n",
       "  release_date     revenue  runtime        spoken_languages    status  \\\n",
       "0   2009-12-10  2787965087    162.0  ['English', 'Español']  Released   \n",
       "\n",
       "                       tagline   title vote_average  vote_count  \n",
       "0  Enter the World of Pandora.  Avatar          7.2       11800  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('myMovies.csv')\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:1' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:2' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:3' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:4' shape=() dtype=int32>,\n",
       " <tf.Tensor 'IteratorGetNext:5' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:6' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:7' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:8' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:9' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:10' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:11' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:12' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:13' shape=() dtype=int64>,\n",
       " <tf.Tensor 'IteratorGetNext:14' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:15' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:16' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:17' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:18' shape=() dtype=string>,\n",
       " <tf.Tensor 'IteratorGetNext:19' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:20' shape=() dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=['myMovies.csv']\n",
    "record_defaults=[[0.0],[0.0],['null'],['null'],[-1],['null'],['en'],['null'],['null'],[0.0],['null'],['null'],['0000-00-00'],[tf.cast(0,tf.int64)],[0.0],['null'],['Released'],['null'],['null'],[0.0],[0.0]]\n",
    "dataset = tf.contrib.data.CsvDataset(filename,record_defaults,header=True)\n",
    "dataset\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 237000000.0, b\"['Action', 'Adventure', 'Fantasy', 'Science Fiction']\", b'http://www.avatarmovie.com/', 19995, b\"['culture clash', 'future', 'space war', 'space colony', 'society', 'space travel', 'futuristic', 'romance', 'space', 'alien', 'tribe', 'alien planet', 'cgi', 'marine', 'soldier', 'battle', 'love affair', 'anti war', 'power relations', 'mind and soul', '3d']\", b'en', b'Avatar', b'In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization.', 150.43758, b\"['Ingenious Film Partners', 'Twentieth Century Fox Film Corporation', 'Dune Entertainment', 'Lightstorm Entertainment']\", b\"['United States of America', 'United Kingdom']\", b'2009-12-10', 2787965087, 162.0, b\"['English', 'Espa\\xc3\\xb1ol']\", b'Released', b'Enter the World of Pandora.', b'Avatar', 7.2, 11800.0)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:1' shape=() dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:2' shape=() dtype=string>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nx,y = tf.placeholder(tf.float32,[None,4]),tf.placeholder(tf.float32,[None,1])\\n\\ndataset=dataset.shuffle(buffer_size=100)\\ntraining = dataset.take(4000)\\ntesting = dataset.skip(800)\\ndataset = tf.data.Dataset.from_tensor_slices((x, y))\\niter = training.make_initializable_iterator()\\nfeatures, labels = iter.get_next()\\nwith tf.Session() as sess:\\n#     initialise iterator with train data\\n    sess.run(iter.initializer, feed_dict={ x: training[0], y: training[1]})\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "x,y = tf.placeholder(tf.float32,[None,4]),tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "dataset=dataset.shuffle(buffer_size=100)\n",
    "training = dataset.take(4000)\n",
    "testing = dataset.skip(800)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "iter = training.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: training[0], y: training[1]})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# initializable iterator to switch between dataset\n",
    "EPOCHS = 10\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32,\n",
    " shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], \n",
    "y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-6574b70507a6>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step 1, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 100, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 300, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 500, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 500\n",
    "batch_size = 10\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 2 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 1 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.softmax(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.nn.softmax(tf.matmul(layer_2, weights['out']) + biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "x, y = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x,batch_y=next_batch(batch_size,x,y)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: np.random.sample((1000,2)),\n",
    "                                      Y:np.random.sample((1000,1))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82098277, 0.0243398 ],\n",
       "       [0.70389191, 0.34277186],\n",
       "       [0.60919606, 0.81199478],\n",
       "       [0.3021588 , 0.08868018],\n",
       "       [0.40426987, 0.66293138],\n",
       "       [0.14693613, 0.81324941],\n",
       "       [0.9644213 , 0.72898304],\n",
       "       [0.11340216, 0.77954666],\n",
       "       [0.72987147, 0.6338782 ],\n",
       "       [0.58816094, 0.89596638],\n",
       "       [0.49561994, 0.08873432],\n",
       "       [0.50039147, 0.35784436],\n",
       "       [0.43170641, 0.87941726],\n",
       "       [0.79600036, 0.20542952],\n",
       "       [0.22755859, 0.92746413],\n",
       "       [0.01097064, 0.19365121],\n",
       "       [0.13856142, 0.85868794],\n",
       "       [0.83127517, 0.54254392],\n",
       "       [0.65279322, 0.04554392],\n",
       "       [0.74267635, 0.89391834]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1, Loss: 0.5910\n",
      "Accurance: Tensor(\"accuracy/value:0\", shape=(), dtype=float32)\n",
      "Iter: 2, Loss: 0.5502\n",
      "Accurance: Tensor(\"accuracy_1/value:0\", shape=(), dtype=float32)\n",
      "Iter: 3, Loss: 0.5108\n",
      "Accurance: Tensor(\"accuracy_2/value:0\", shape=(), dtype=float32)\n",
      "Iter: 4, Loss: 0.4728\n",
      "Accurance: Tensor(\"accuracy_3/value:0\", shape=(), dtype=float32)\n",
      "Iter: 5, Loss: 0.4363\n",
      "Accurance: Tensor(\"accuracy_4/value:0\", shape=(), dtype=float32)\n",
      "Iter: 6, Loss: 0.4015\n",
      "Accurance: Tensor(\"accuracy_5/value:0\", shape=(), dtype=float32)\n",
      "Iter: 7, Loss: 0.3686\n",
      "Accurance: Tensor(\"accuracy_6/value:0\", shape=(), dtype=float32)\n",
      "Iter: 8, Loss: 0.3375\n",
      "Accurance: Tensor(\"accuracy_7/value:0\", shape=(), dtype=float32)\n",
      "Iter: 9, Loss: 0.3085\n",
      "Accurance: Tensor(\"accuracy_8/value:0\", shape=(), dtype=float32)\n",
      "Iter: 10, Loss: 0.2815\n",
      "Accurance: Tensor(\"accuracy_9/value:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat()\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "x, y = iter.get_next()\n",
    "# make a simple model\n",
    "net = tf.layers.dense(x, 20, activation=tf.tanh) # pass the first value \n",
    "#from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 10, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value \n",
    "#from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i+1, loss_value)) \n",
    "        acc, acc_op =tf.metrics.accuracy(predictions=prediction,labels=y)\n",
    "        print(\"Accurance: \" +str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.81143049],\n",
       "        [0.5155631 ],\n",
       "        [0.82289195],\n",
       "        [0.28533676],\n",
       "        [0.17927069],\n",
       "        [0.23536197],\n",
       "        [0.05036924],\n",
       "        [0.10474851],\n",
       "        [0.41743359],\n",
       "        [0.67871757],\n",
       "        [0.37220044],\n",
       "        [0.86174792],\n",
       "        [0.45655657],\n",
       "        [0.11947828],\n",
       "        [0.37956098],\n",
       "        [0.0376108 ],\n",
       "        [0.32591053],\n",
       "        [0.31365506],\n",
       "        [0.59372349],\n",
       "        [0.99775987],\n",
       "        [0.55374745],\n",
       "        [0.08404339],\n",
       "        [0.87897145],\n",
       "        [0.21590492],\n",
       "        [0.22001246],\n",
       "        [0.89417126],\n",
       "        [0.73037409],\n",
       "        [0.48820147],\n",
       "        [0.86326412],\n",
       "        [0.84911492],\n",
       "        [0.72910994],\n",
       "        [0.53609143],\n",
       "        [0.89089954],\n",
       "        [0.80715674],\n",
       "        [0.99826225],\n",
       "        [0.68351576],\n",
       "        [0.18896207],\n",
       "        [0.38923251],\n",
       "        [0.8717949 ],\n",
       "        [0.79548879],\n",
       "        [0.21026406],\n",
       "        [0.65054501],\n",
       "        [0.58757723],\n",
       "        [0.47316996],\n",
       "        [0.59320941],\n",
       "        [0.04553989],\n",
       "        [0.17066693],\n",
       "        [0.84437576],\n",
       "        [0.90353819],\n",
       "        [0.67704144],\n",
       "        [0.64380523],\n",
       "        [0.25465971],\n",
       "        [0.6198218 ],\n",
       "        [0.29599428],\n",
       "        [0.36891985],\n",
       "        [0.56005213],\n",
       "        [0.67703926],\n",
       "        [0.93196467],\n",
       "        [0.74898872],\n",
       "        [0.55944002],\n",
       "        [0.38916053],\n",
       "        [0.5687442 ],\n",
       "        [0.65915616],\n",
       "        [0.01297305],\n",
       "        [0.65296321],\n",
       "        [0.26699924],\n",
       "        [0.34024034],\n",
       "        [0.82696877],\n",
       "        [0.38545879],\n",
       "        [0.32944396],\n",
       "        [0.413173  ],\n",
       "        [0.11811752],\n",
       "        [0.96907967],\n",
       "        [0.19878992],\n",
       "        [0.39416489],\n",
       "        [0.52281527],\n",
       "        [0.04492218],\n",
       "        [0.874861  ],\n",
       "        [0.31961282],\n",
       "        [0.25160814],\n",
       "        [0.39774117],\n",
       "        [0.04017014],\n",
       "        [0.69600057],\n",
       "        [0.40873458],\n",
       "        [0.510742  ],\n",
       "        [0.50677792],\n",
       "        [0.65925588],\n",
       "        [0.83343283],\n",
       "        [0.6332819 ],\n",
       "        [0.1224188 ],\n",
       "        [0.24405251],\n",
       "        [0.93331369],\n",
       "        [0.80594735],\n",
       "        [0.85040221],\n",
       "        [0.06986151],\n",
       "        [0.3388621 ],\n",
       "        [0.86624162],\n",
       "        [0.01028986],\n",
       "        [0.27293671],\n",
       "        [0.17684333]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
